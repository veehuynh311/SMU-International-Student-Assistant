{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéì SMU International Student Assistant \n",
        "\n",
        "**Project:** RAG Assistant for F1 International Students at Southern Methodist University (SMU)\n",
        "\n",
        "**Author:** Vee Huynh\n",
        "\n",
        "**Date:** February 2026\n",
        "\n",
        "**GitHub:** https://github.com/veehuynh311/SMU-International-Student-Assistant\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Problem Statement\n",
        "\n",
        "**Domain:** F1 Visa Navigation for International Students at Southern Methodist University (SMU)\n",
        "\n",
        "**Target User:** F1 international students at SMU, especially new arrivals and those seeking employment authorization (internships, post-graduation jobs).\n",
        "\n",
        "**Problem:** International students face a maze of complex regulations across multiple life areas: maintaining F1 status, work authorization (CPT/OPT/STEM OPT), filing taxes, obtaining an SSN, and getting a Texas driver's license. Information is scattered across 6+ government agencies (USCIS, IRS, DHS, SSA, Texas DPS) and university resources. One mistake can jeopardize visa status, leading to serious consequences including deportation. This RAG assistant consolidates 12 official documents into a single conversational interface, providing accurate, source-cited answers to questions like \"How do I apply for CPT?\", \"Will full-time CPT affect my OPT eligibility?\", \"How do I get an SSN?\", or \"What documents do I need for a Texas driver's license?\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages for document loading and text processing\n",
        "!pip install pypdf beautifulsoup4 lxml langchain -q\n",
        "print(\"‚úÖ Packages installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Step 2: Upload My Documents\n",
        "\n",
        "Upload documents that i collected (PDF, HTML files).\n",
        "\n",
        "**Documents collected for this project (12 total):**\n",
        "\n",
        "| # | Source | Topic | Type | Filename |\n",
        "|---|--------|-------|------|----------|\n",
        "| 1 | SMU ISSS | New Student Information | HTML | `smu_new_student_info.html` |\n",
        "| 2 | SMU ISSS | Current Student Information | HTML | `smu_current_student_info.html` |\n",
        "| 3 | SMU ISSS | US Living (Tax, SSN, DL) | HTML | `smu_us_living.html` |\n",
        "| 4 | USCIS | OPT for F-1 Students | HTML | `uscis_opt.html` |\n",
        "| 5 | USCIS | STEM OPT Extension | HTML | `uscis_stem_opt.html` |\n",
        "| 6 | DHS | CPT Guide | HTML | `dhs_cpt_guide.html` |\n",
        "| 7 | ICE | Practical Training | HTML | `ice_practical_training.html` |\n",
        "| 8 | IRS | Form 8843 Instructions | HTML | `irs_form_8843.html` |\n",
        "| 9 | Sprintax | F1 Tax Guide | HTML | `sprintax_tax_guide.html` |\n",
        "| 10 | DHS | Obtaining SSN | HTML | `dhs_ssn_guide.html` |\n",
        "| 11 | SSA | SSN for International Students | PDF | `ssa_international_students_ssn.pdf` |\n",
        "| 12 | Texas DPS | Driver License Checklist | PDF | `texas_dps_dl_checklist.pdf` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create documents folder\n",
        "os.makedirs(\"documents\", exist_ok=True)\n",
        "\n",
        "print(\"üìÅ Upload documents (PDF, HTML files):\")\n",
        "print(\"   Target: 12 documents\\n\")\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move to documents folder\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, f\"documents/{filename}\")\n",
        "    print(f\"   ‚úÖ Saved: documents/{filename}\")\n",
        "\n",
        "print(f\"\\nüìä Total documents uploaded: {len(uploaded)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Step 3: List All Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "doc_files = os.listdir(\"documents\")\n",
        "print(f\"üìÇ Documents in folder ({len(doc_files)} files):\\n\")\n",
        "\n",
        "for i, filename in enumerate(doc_files, 1):\n",
        "    filepath = f\"documents/{filename}\"\n",
        "    size_kb = os.path.getsize(filepath) / 1024\n",
        "    extension = filename.split('.')[-1].upper()\n",
        "    print(f\"   {i}. [{extension}] {filename} ({size_kb:.1f} KB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìñ PART 1: Data Ingestion\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 4: Define Document Loaders\n",
        "\n",
        "Different functions to load different file types:\n",
        "- **PDF**: Use `pypdf` to extract text with page tracking\n",
        "- **HTML**: Use `BeautifulSoup` to parse and extract text\n",
        "- **TXT**: Simple file read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pypdf import PdfReader\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def load_pdf(filepath):\n",
        "    \"\"\"\n",
        "    Load a PDF file and extract text from all pages.\n",
        "    Returns tuple: (full_text, pages_list)\n",
        "    where pages_list contains (page_num, start_char, end_char) for metadata.\n",
        "    \"\"\"\n",
        "    reader = PdfReader(filepath)\n",
        "    full_text = \"\"\n",
        "    pages_list = []  # List of(page_num, start_char, end_char)\n",
        "\n",
        "    for page_num, page in enumerate(reader.pages, start=1):\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            start_char = len(full_text)\n",
        "            full_text += page_text + \"\\n\"\n",
        "            end_char = len(full_text)\n",
        "            pages_list.append((page_num, start_char, end_char))\n",
        "\n",
        "    return full_text, pages_list\n",
        "\n",
        "\n",
        "def load_html(filepath):\n",
        "    \"\"\"\n",
        "    Load an HTML file and extract text content.\n",
        "    Returns tuple: (text, None) - no page tracking for HTML.\n",
        "    \"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        html_content = f.read()\n",
        "\n",
        "    soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "    # Remove non-content elements\n",
        "    for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
        "        element.decompose()\n",
        "\n",
        "    text = soup.get_text(separator='\\n')\n",
        "    return text, None\n",
        "\n",
        "\n",
        "def load_txt(filepath):\n",
        "    \"\"\"\n",
        "    Load a plain text file.\n",
        "    Returns tuple: (text, None)\n",
        "    \"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        return f.read(), None\n",
        "\n",
        "\n",
        "def load_document(filepath):\n",
        "    \"\"\"\n",
        "    Load a document based on its file extension.\n",
        "    Returns tuple: (text, page_info)\n",
        "    \"\"\"\n",
        "    extension = filepath.lower().split('.')[-1]\n",
        "\n",
        "    if extension == 'pdf':\n",
        "        return load_pdf(filepath)\n",
        "    elif extension in ['html', 'htm']:\n",
        "        return load_html(filepath)\n",
        "    elif extension == 'txt':\n",
        "        return load_txt(filepath)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Unknown file type: {extension}\")\n",
        "        return \"\", None\n",
        "\n",
        "\n",
        "def get_doc_type(filename):\n",
        "    \"\"\"\n",
        "    Determine document type/category based on filename.\n",
        "    \"\"\"\n",
        "    filename_lower = filename.lower()\n",
        "    if 'smu' in filename_lower:\n",
        "        return 'university'\n",
        "    elif 'uscis' in filename_lower or 'dhs' in filename_lower or 'ice' in filename_lower:\n",
        "        return 'immigration'\n",
        "    elif 'irs' in filename_lower or 'tax' in filename_lower or 'sprintax' in filename_lower:\n",
        "        return 'tax'\n",
        "    elif 'ssa' in filename_lower or 'ssn' in filename_lower:\n",
        "        return 'ssn'\n",
        "    elif 'dps' in filename_lower or 'driver' in filename_lower:\n",
        "        return 'driver_license'\n",
        "    else:\n",
        "        return 'general'\n",
        "\n",
        "print(\"‚úÖ Document loader functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ Step 5: Define Text Cleaning Function\n",
        "\n",
        "Clean up common issues:\n",
        "- Extra whitespace and newlines\n",
        "- Headers/footers\n",
        "- Special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean extracted text by removing noise.\n",
        "    \"\"\"\n",
        "    # Replace multiple newlines with double newline\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "\n",
        "    # Replace multiple spaces with single space\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "\n",
        "    # Remove leading/trailing whitespace from each line\n",
        "    lines = [line.strip() for line in text.split('\\n')]\n",
        "\n",
        "    # Remove empty lines\n",
        "    lines = [line for line in lines if line]\n",
        "\n",
        "    text = '\\n'.join(lines)\n",
        "\n",
        "    # Remove common noise patterns\n",
        "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Final cleanup of whitespace\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "print(\"‚úÖ Text cleaning function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìñ Step 6: Load and Clean All Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Store documents with metadata\n",
        "documents = []\n",
        "\n",
        "print(\"üìñ Loading and cleaning documents...\\n\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for filename in sorted(os.listdir(\"documents\")):\n",
        "    filepath = f\"documents/{filename}\"\n",
        "\n",
        "    try:\n",
        "        # Load the document (returns text and page info for PDFs)\n",
        "        raw_text, page_info = load_document(filepath)\n",
        "\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "\n",
        "        # Determine document type\n",
        "        doc_type = get_doc_type(filename)\n",
        "        file_type = filename.split('.')[-1].upper()\n",
        "\n",
        "        # Store document info with enhanced metadata\n",
        "        doc = {\n",
        "            \"filename\": filename,\n",
        "            \"filepath\": filepath,\n",
        "            \"source\": filename.replace(\"_\", \" \").replace(\".html\", \"\").replace(\".pdf\", \"\").replace(\".txt\", \"\"),\n",
        "            \"doc_type\": doc_type,        # Category: immigration, tax, ssn, etc.\n",
        "            \"file_type\": file_type,      # PDF, HTML, TXT\n",
        "            \"page_info\": page_info,      # For PDFs: list of(page_num, start, end)\n",
        "            \"raw_length\": len(raw_text),\n",
        "            \"cleaned_length\": len(cleaned_text),\n",
        "            \"text\": cleaned_text\n",
        "        }\n",
        "        documents.append(doc)\n",
        "\n",
        "        reduction = (1 - len(cleaned_text)/len(raw_text)) * 100 if len(raw_text) > 0 else 0\n",
        "        page_str = f\", {len(page_info)} pages\" if page_info else \"\"\n",
        "        print(f\"‚úÖ {filename} [{doc_type}]\")\n",
        "        print(f\"   Raw: {len(raw_text):,} chars ‚Üí Cleaned: {len(cleaned_text):,} chars ({reduction:.0f}% reduction){page_str}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {filename}: {e}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"\\nüìä Successfully loaded {len(documents)} documents!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üëÄ Step 7: Display Sample Cleaned Text\n",
        "\n",
        "Preview each document to verify extraction worked correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"üìÑ SAMPLE CLEANED TEXT FROM EACH DOCUMENT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, doc in enumerate(documents[:5], 1):  # Show first 5 docs\n",
        "    print(f\"\\n{'‚îÄ' * 70}\")\n",
        "    print(f\"üìÑ Document {i}: {doc['filename']}\")\n",
        "    print(f\"   Type: {doc['doc_type']} | Format: {doc['file_type']}\")\n",
        "    print(f\"   Total length: {doc['cleaned_length']:,} characters\")\n",
        "    print(f\"{'‚îÄ' * 70}\")\n",
        "\n",
        "    # Show first 800 characters as preview\n",
        "    preview = doc['text'][:800]\n",
        "    if len(doc['text']) > 800:\n",
        "        preview += \"\\n\\n[... truncated ...]\"\n",
        "\n",
        "    print(preview)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ Session 2 Complete: Text extraction working!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ‚úÇÔ∏è PART 2: Chunking\n",
        "\n",
        "---\n",
        "\n",
        "## Why Chunking?\n",
        "\n",
        "Documents are too long to fit in an LLM's context window and too broad for accurate retrieval. We split them into smaller, focused chunks.\n",
        "\n",
        "**Settings (500-800 chars):**\n",
        "- `chunk_size`: 600 characters\n",
        "- `chunk_overlap`: 100 characters (to preserve context at boundaries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÇÔ∏è Step 8: Define Chunking Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize the text splitter with specified settings\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=600,           # Target chunk size in characters (500-800 range)\n",
        "    chunk_overlap=100,        # Overlap between chunks\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Priority order for splitting\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Text splitter configured!\")\n",
        "print(f\"   ‚Ä¢ chunk_size: 600 characters\")\n",
        "print(f\"   ‚Ä¢ chunk_overlap: 100 characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÇÔ∏è Step 9: Chunk All Documents with Enhanced Metadata\n",
        "\n",
        "**Metadata includes:**\n",
        "- Document name (source)\n",
        "- Page/section (for PDFs)\n",
        "- Document type category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_page_for_position(position, page_info):\n",
        "    \"\"\"\n",
        "    Given a character position in text, return the page number.\n",
        "    For PDFs only - returns None for HTML/TXT.\n",
        "    \"\"\"\n",
        "    if not page_info:\n",
        "        return None\n",
        "    for page_num, start, end in page_info:\n",
        "        if start <= position < end:\n",
        "            return page_num\n",
        "    return page_info[-1][0] if page_info else None  # Default to last page\n",
        "\n",
        "\n",
        "# Store all chunks with metadata\n",
        "all_chunks = []\n",
        "\n",
        "print(\"‚úÇÔ∏è Chunking documents with enhanced metadata...\\n\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for doc in documents:\n",
        "    # Split the document text into chunks\n",
        "    chunks = text_splitter.split_text(doc['text'])\n",
        "\n",
        "    # Track position in original text for page mapping\n",
        "    current_pos = 0\n",
        "\n",
        "    # Add metadata to each chunk\n",
        "    for i, chunk_text in enumerate(chunks):\n",
        "        # Find chunk position in cleaned text (approximate)\n",
        "        chunk_start = doc['text'].find(chunk_text[:50], current_pos)\n",
        "        if chunk_start == -1:\n",
        "            chunk_start = current_pos\n",
        "\n",
        "        # Get page number for this chunk (PDFs only)\n",
        "        page_num = get_page_for_position(chunk_start, doc['page_info'])\n",
        "\n",
        "        chunk = {\n",
        "            \"text\": chunk_text,\n",
        "            \"metadata\": {\n",
        "                \"source\": doc['source'],           # Document name\n",
        "                \"filename\": doc['filename'],\n",
        "                \"doc_type\": doc['doc_type'],       # Category (immigration, tax, etc.)\n",
        "                \"file_type\": doc['file_type'],     # PDF, HTML\n",
        "                \"page\": page_num,                  # Page number (PDFs only)\n",
        "                \"chunk_id\": i,\n",
        "                \"total_chunks\": len(chunks)\n",
        "            }\n",
        "        }\n",
        "        all_chunks.append(chunk)\n",
        "        current_pos = chunk_start + len(chunk_text) - 100  # Account for overlap\n",
        "\n",
        "    page_str = f\" (pages tracked)\" if doc['page_info'] else \"\"\n",
        "    print(f\"‚úÖ {doc['filename']}\")\n",
        "    print(f\"   {doc['cleaned_length']:,} chars ‚Üí {len(chunks)} chunks{page_str}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"\\nüìä Total chunks created: {len(all_chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 10: Compute and Log Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "# Calculate statistics\n",
        "chunk_lengths = [len(chunk['text']) for chunk in all_chunks]\n",
        "\n",
        "total_chunks = len(all_chunks)\n",
        "avg_length = statistics.mean(chunk_lengths)\n",
        "min_length = min(chunk_lengths)\n",
        "max_length = max(chunk_lengths)\n",
        "std_length = statistics.stdev(chunk_lengths) if len(chunk_lengths) > 1 else 0\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä CHUNKING STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìà Overall Statistics:\")\n",
        "print(f\"   ‚Ä¢ Total documents: {len(documents)}\")\n",
        "print(f\"   ‚Ä¢ Total chunks: {total_chunks}\")\n",
        "print(f\"   ‚Ä¢ Average chunk length: {avg_length:.0f} characters\")\n",
        "print(f\"   ‚Ä¢ Min chunk length: {min_length} characters\")\n",
        "print(f\"   ‚Ä¢ Max chunk length: {max_length} characters\")\n",
        "print(f\"   ‚Ä¢ Std deviation: {std_length:.0f} characters\")\n",
        "\n",
        "print(f\"\\nüìÑ Chunks per Document:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Count chunks per document\n",
        "chunks_per_doc = {}\n",
        "for chunk in all_chunks:\n",
        "    filename = chunk['metadata']['filename']\n",
        "    chunks_per_doc[filename] = chunks_per_doc.get(filename, 0) + 1\n",
        "\n",
        "for filename, count in chunks_per_doc.items():\n",
        "    print(f\"   ‚Ä¢ {filename}: {count} chunks\")\n",
        "\n",
        "# Count by document type\n",
        "print(f\"\\nüìÇ Chunks by Document Type:\")\n",
        "print(\"-\" * 60)\n",
        "chunks_per_type = {}\n",
        "for chunk in all_chunks:\n",
        "    doc_type = chunk['metadata']['doc_type']\n",
        "    chunks_per_type[doc_type] = chunks_per_type.get(doc_type, 0) + 1\n",
        "\n",
        "for doc_type, count in sorted(chunks_per_type.items()):\n",
        "    print(f\"   ‚Ä¢ {doc_type}: {count} chunks\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üëÄ Step 11: Print 3-5 Sample Chunks with Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"üìù SAMPLE CHUNKS WITH METADATA (5 examples)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Select diverse sample chunks (from different documents)\n",
        "sample_indices = [0, len(all_chunks)//4, len(all_chunks)//2, 3*len(all_chunks)//4, len(all_chunks)-1]\n",
        "sample_indices = sample_indices[:5]  # Ensure max 5\n",
        "\n",
        "for idx in sample_indices:\n",
        "    chunk = all_chunks[idx]\n",
        "    meta = chunk['metadata']\n",
        "    print(f\"\\n{'‚îÄ' * 70}\")\n",
        "    print(f\"üìå Chunk #{idx}\")\n",
        "    print(f\"   Source: {meta['source']}\")\n",
        "    print(f\"   Type: {meta['doc_type']} | Format: {meta['file_type']}\")\n",
        "    page_str = f\" | Page: {meta['page']}\" if meta['page'] else \"\"\n",
        "    print(f\"   Chunk: {meta['chunk_id'] + 1} of {meta['total_chunks']}{page_str}\")\n",
        "    print(f\"   Length: {len(chunk['text'])} characters\")\n",
        "    print(f\"{'‚îÄ' * 70}\")\n",
        "    print(chunk['text'][:500])\n",
        "    if len(chunk['text']) > 500:\n",
        "        print(\"\\n[... truncated ...]\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ Session 3 Complete: Chunking with enhanced metadata working!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 12: Save Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save chunks to JSON for next session\n",
        "with open('chunks.json', 'w') as f:\n",
        "    json.dump(all_chunks, f, indent=2)\n",
        "\n",
        "print(\"üíæ Chunks saved to 'chunks.json'\")\n",
        "\n",
        "# Save statistics\n",
        "stats = {\n",
        "    \"total_documents\": len(documents),\n",
        "    \"total_chunks\": total_chunks,\n",
        "    \"avg_chunk_length\": round(avg_length, 2),\n",
        "    \"min_chunk_length\": min_length,\n",
        "    \"max_chunk_length\": max_length,\n",
        "    \"chunk_size_setting\": 600,\n",
        "    \"chunk_overlap_setting\": 100,\n",
        "    \"chunks_per_document\": chunks_per_doc,\n",
        "    \"chunks_per_type\": chunks_per_type\n",
        "}\n",
        "\n",
        "with open('chunking_stats.json', 'w') as f:\n",
        "    json.dump(stats, f, indent=2)\n",
        "\n",
        "print(\"üíæ Statistics saved to 'chunking_stats.json'\")\n",
        "\n",
        "# Download files\n",
        "files.download('chunks.json')\n",
        "files.download('chunking_stats.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Summary: Sessions 2 & 3 Complete!\n",
        "\n",
        "### Session 2 (Data Ingestion):\n",
        "- ‚úÖ Loaded 12 documents (PDF, HTML)\n",
        "- ‚úÖ Extracted and cleaned text\n",
        "- ‚úÖ Printed sample cleaned text\n",
        "\n",
        "### Session 3 (Chunking):\n",
        "- ‚úÖ Implemented chunking (chunk_size=600, overlap=100)\n",
        "- ‚úÖ **Enhanced metadata:** doc name, page (PDFs), doc_type category\n",
        "- ‚úÖ Computed statistics: total chunks, avg length, chunks per doc/type\n",
        "- ‚úÖ Printed 3-5 sample chunks with metadata\n",
        "- ‚úÖ Saved data for next session\n",
        "\n",
        "### Topics Covered by Documents:\n",
        "| Topic | Sources | Doc Type |\n",
        "|-------|--------|----------|\n",
        "| F1 Status | SMU ISSS (3 docs) | university |\n",
        "| CPT | DHS, ICE | immigration |\n",
        "| OPT | USCIS, ICE | immigration |\n",
        "| STEM OPT | USCIS | immigration |\n",
        "| Taxes | IRS, Sprintax | tax |\n",
        "| SSN | DHS, SSA, SMU | ssn |\n",
        "| Texas Driver's License | Texas DPS, SMU | driver_license |\n",
        "\n",
        "### Next Steps (Session 4):\n",
        "- Generate embeddings using sentence-transformers (all-MiniLM-L6-v2)\n",
        "- Build vector store with FAISS\n",
        "- Implement `retrieve_top_k(query)` function\n",
        "- Write 10-15 test questions and inspect retrieval results\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
